{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "dd4482cb-3ca0-4eae-89fd-4b9df1698980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be very descriptive about how the makemore2 works. We will get more into pytorch within this\n",
    "# for this notebook, I am not going to add in the train and test set splitting. I will do so in the future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ff6fbadd-15c5-4652-99ad-387eda1c37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a400a384-a1e3-4eae-9160-229f923be530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all words\n",
    "words= open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8c4d90e3-e10c-4508-a412-ccf708485c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c75af15e-726f-4963-8c78-a3d685d04eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mapping to/from integers\n",
    "chars= sorted(list(set(''.join(words))))\n",
    "stoi= {s:i+1 for i,s in enumerate(chars)}\n",
    "# build special characters for the start and end\n",
    "stoi['.'] = 0\n",
    "itos= {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a582d723-51dd-4a1f-9974-f7544d4aa990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... --> e\n",
      "..e --> m\n",
      ".em --> m\n",
      "emm --> a\n",
      "mma --> .\n",
      "olivia\n",
      "... --> o\n",
      "..o --> l\n",
      ".ol --> i\n",
      "oli --> v\n",
      "liv --> i\n",
      "ivi --> a\n",
      "via --> .\n",
      "ava\n",
      "... --> a\n",
      "..a --> v\n",
      ".av --> a\n",
      "ava --> .\n",
      "isabella\n",
      "... --> i\n",
      "..i --> s\n",
      ".is --> a\n",
      "isa --> b\n",
      "sab --> e\n",
      "abe --> l\n",
      "bel --> l\n",
      "ell --> a\n",
      "lla --> .\n",
      "sophia\n",
      "... --> s\n",
      "..s --> o\n",
      ".so --> p\n",
      "sop --> h\n",
      "oph --> i\n",
      "phi --> a\n",
      "hia --> .\n"
     ]
    }
   ],
   "source": [
    "# to build a dataset\n",
    "\n",
    "# consider that we only want a block size or starting context to predict each letter\n",
    "blockSize= 3\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    # initialize the context to be of size blockSize and nothing inside\n",
    "    context= [0] * blockSize\n",
    "    # for each character, we want to construct the context used to find answer or predicted character\n",
    "    # we are trying to build something like this\n",
    "    # ['.', '.', '.'] is used to predict ['a']\n",
    "    # and so forth\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        # we add into the context data set used for prediction (in numbers)\n",
    "        X.append(context)\n",
    "        # and then we also add the related answer in integer format\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "        # then we need to consider updating the context to prepare what the context is ued for next prediction\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X= torch.tensor(X)\n",
    "Y= torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eeba7566-8880-4f66-899f-a3abaad7e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to get into the context (X) and how to predict the Y\n",
    "# we need to start out with some embedding lookup table\n",
    "\n",
    "# he were are going to condense the 27 letters into 2 dimensional space\n",
    "C = torch.rand((27,2))\n",
    "\n",
    "# this means: we have 2 attributes/values that is used to encode a letter/integer for the letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6de4f986-502f-400f-a2ec-0011540e7e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the first 5 words, we will have X.shape examples to go from alongside Y.shape outputs/answers\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f66d9cd-015a-47e4-97ed-6159f95b871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.8987, 0.0592]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.9669, 0.3707]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.9669, 0.3707],\n",
       "         [0.1094, 0.9712]],\n",
       "\n",
       "        [[0.9669, 0.3707],\n",
       "         [0.1094, 0.9712],\n",
       "         [0.1094, 0.9712]],\n",
       "\n",
       "        [[0.1094, 0.9712],\n",
       "         [0.1094, 0.9712],\n",
       "         [0.8704, 0.5595]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.8987, 0.0592]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.2211, 0.2046]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.2211, 0.2046],\n",
       "         [0.2085, 0.8658]],\n",
       "\n",
       "        [[0.2211, 0.2046],\n",
       "         [0.2085, 0.8658],\n",
       "         [0.4294, 0.8042]],\n",
       "\n",
       "        [[0.2085, 0.8658],\n",
       "         [0.4294, 0.8042],\n",
       "         [0.5238, 0.3325]],\n",
       "\n",
       "        [[0.4294, 0.8042],\n",
       "         [0.5238, 0.3325],\n",
       "         [0.4294, 0.8042]],\n",
       "\n",
       "        [[0.5238, 0.3325],\n",
       "         [0.4294, 0.8042],\n",
       "         [0.8704, 0.5595]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.8987, 0.0592]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.8704, 0.5595]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8704, 0.5595],\n",
       "         [0.5238, 0.3325]],\n",
       "\n",
       "        [[0.8704, 0.5595],\n",
       "         [0.5238, 0.3325],\n",
       "         [0.8704, 0.5595]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.8987, 0.0592]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.4294, 0.8042]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.4294, 0.8042],\n",
       "         [0.9219, 0.5195]],\n",
       "\n",
       "        [[0.4294, 0.8042],\n",
       "         [0.9219, 0.5195],\n",
       "         [0.8704, 0.5595]],\n",
       "\n",
       "        [[0.9219, 0.5195],\n",
       "         [0.8704, 0.5595],\n",
       "         [0.7918, 0.1613]],\n",
       "\n",
       "        [[0.8704, 0.5595],\n",
       "         [0.7918, 0.1613],\n",
       "         [0.9669, 0.3707]],\n",
       "\n",
       "        [[0.7918, 0.1613],\n",
       "         [0.9669, 0.3707],\n",
       "         [0.2085, 0.8658]],\n",
       "\n",
       "        [[0.9669, 0.3707],\n",
       "         [0.2085, 0.8658],\n",
       "         [0.2085, 0.8658]],\n",
       "\n",
       "        [[0.2085, 0.8658],\n",
       "         [0.2085, 0.8658],\n",
       "         [0.8704, 0.5595]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.8987, 0.0592]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.8987, 0.0592],\n",
       "         [0.9219, 0.5195]],\n",
       "\n",
       "        [[0.8987, 0.0592],\n",
       "         [0.9219, 0.5195],\n",
       "         [0.2211, 0.2046]],\n",
       "\n",
       "        [[0.9219, 0.5195],\n",
       "         [0.2211, 0.2046],\n",
       "         [0.9875, 0.4379]],\n",
       "\n",
       "        [[0.2211, 0.2046],\n",
       "         [0.9875, 0.4379],\n",
       "         [0.0410, 0.6720]],\n",
       "\n",
       "        [[0.9875, 0.4379],\n",
       "         [0.0410, 0.6720],\n",
       "         [0.4294, 0.8042]],\n",
       "\n",
       "        [[0.0410, 0.6720],\n",
       "         [0.4294, 0.8042],\n",
       "         [0.8704, 0.5595]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can index into the embedding table normally like\n",
    "C[5]\n",
    "# or through a tensor\n",
    "C[torch.tensor(5)]\n",
    "# or even through a larger tensor, we can use the example for how the examples\n",
    "# are shown through the embeddings\n",
    "# basically, using the embeddings tables, showcase the context with the embedded values for each character\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f3612df2-43d6-463f-b622-57b00a2d251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "18eb68f5-5546-42dc-a6c0-9647884dba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c9b4be9-7da1-4f06-b891-e6000f62c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can choose the 'second' layer or number of neurons - in this case we chose 100\n",
    "W1 = torch.randn((6,100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f1a7cd8-c9ae-4117-8c17-3d05229919d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We would need to do the embedding .matmul or @ with the weights\n",
    "# PROBLEM: we can't do that right now since emb is of shape 32x3x2 and W1 is 6x100\n",
    "# matrix multiplication won't work with this it needs to be 32x6 @ 6x100\n",
    "\n",
    "# what we can do is turn the emb into 32x6\n",
    "\n",
    "# METHOD 1:\n",
    "# concatenate each of the second (1) dimension (which contains the embeddings) and retaining the first dimension (0)\n",
    "torch.cat([emb[:,0, :], emb[:, 0, :], emb[:, 0, :]], 1).shape\n",
    "\n",
    "# METHOD 2:\n",
    "# do with torch.unbind - I am imagining this as unfurling everything against a certain dimension and breaking barriers\n",
    "torch.cat(torch.unbind(emb, 1), 1).shape\n",
    "\n",
    "# METHOD 3: easily done with view. also -1 is to assume the actual first dimension (0)\n",
    "# which is the same as emb.view(32, 6).shape\n",
    "# emb.view(emb.shape[0], 6).shape\n",
    "emb.view(-1,6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "574e4b1e-7b0e-41e4-aac2-4d4c89befe0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = emb.view(-1,6) @ W1 + b1\n",
    "\n",
    "# now that we can matrix multiply them, we can now get into the actual from input to second layer (100)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d8944d2-a6fb-4d39-b46f-35913255b349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3712, -0.9268, -0.5447,  ..., -0.1120,  0.0221, -0.7995],\n",
       "        [-0.6500, -0.9487, -0.3599,  ...,  0.0135,  0.0153, -0.6403],\n",
       "        [-0.8691, -0.9523,  0.3634,  ...,  0.2273,  0.0426, -0.1092],\n",
       "        ...,\n",
       "        [-0.9811, -0.9593,  0.4293,  ...,  0.3320,  0.3126,  0.1718],\n",
       "        [-0.9443, -0.9898, -0.9082,  ...,  0.0523, -0.4966,  0.2368],\n",
       "        [ 0.1598, -0.9980, -0.5527,  ..., -0.7439,  0.4646, -0.9241]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but we aren't done, we need to make this an activation layer with tanh\n",
    "# tanh is an activation method - to detemine how much inputs are activating neurons to help determine an output\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed28495b-82d1-41b7-8156-d65a0d2256d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there is something that is happening here that needs to be considered, check out Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60a9237e-af5e-47f3-bd4e-f7622b9ff86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can consider the final output layer, basically the predictions that we want to come out with\n",
    "W2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2078f4cc-dabb-492b-8b5c-54ccbff6a9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits are the raw scores that come out - we will need to normalize them later\n",
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "473bf0b8-1ea0-4706-a9da-a355ed14332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to noralize these outputs\n",
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9111323-d138-4574-8157-628e2e252aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can provide the probabilities after normalizing the results\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e899c509-87ed-4399-b352-3e0828bfca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8326e-08, 3.8800e-01, 3.1822e-01, 2.6593e-12, 1.3562e-07, 1.6771e-14,\n",
       "        2.4845e-06, 3.8503e-11, 1.1537e-04, 1.8708e-10, 1.0023e-15, 6.5709e-08,\n",
       "        2.8670e-15, 7.3632e-07, 1.8952e-15, 5.9944e-09, 1.0149e-09, 4.7629e-07,\n",
       "        4.0095e-17, 3.5814e-08, 3.5856e-06, 4.9048e-06, 5.4525e-06, 1.6544e-15,\n",
       "        2.5030e-07, 3.2234e-08, 6.5850e-14, 1.4592e-14, 1.2569e-09, 3.4408e-09,\n",
       "        3.1143e-14, 3.0366e-07])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will now need to calculate the loss but first we want to check out the probably in relation to our outputs\n",
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c8313e8d-e9bb-47d3-a986-d0bc299e0c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.4705)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can the log probability and then the average of them to find the negative log likelihood loss\n",
    "# LOSS\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc664db4-2eb4-432c-b050-ca2c5bd53b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### THIS SECTION IS REORGANIZING EVERYTHING TO MAKE IT LOOK NICER AND EASIER TO FOLLOW #####\n",
    "X.shape, Y.shape ## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b0a6e03a-6885-473f-9011-cb3c78d47899",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is for reproducability\n",
    "# Embedding matrix mapping each of the 27 possible characters to a 2D vector representation\n",
    "C = torch.randn((27,2), generator=g)\n",
    "# Weights and biases for the first linear layer: input of 6 (from 3 chars × 2 embedding dim) → 100 hidden units\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "\n",
    "# Weights and biases for output layer: 100 → 27 (each output logit corresponds to one possible next character).\n",
    "W2= torch.randn((100, 27), generator=g)\n",
    "b2= torch.randn(27, generator=g)\n",
    "\n",
    "# just to organize it all together\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "117da296-6ab0-4d5d-9bec-92e9cc1801f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also see how many parameters there are\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "86244572-fa82-4d40-b242-ced621293fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up embeddings for each of the 3 input characters in every one of the 32 examples. Shape becomes (32, 3, 2)\n",
    "emb= C[X]\n",
    "\n",
    "# Flatten the 3×2 embeddings (per example) into a 1D vector of 6 pieces with view,\n",
    "# then apply the first linear layer and tanh activation to produce 100 hidden activations per example\n",
    "# h is representation of the hidden activations for the 32 examples\n",
    "h= torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "\n",
    "# get the raw scores on output, so another layer of calculations\n",
    "# Transform 100 hidden activations into 27 logits — one unnormalized score per possible output character.\n",
    "logits= h @ W2 + b2\n",
    "\n",
    "############## THIS SECTION BELOW WITH COUNTS, PROB, AND LOSS CAN BE DONE THROUGH Functional.cross_entropy(logits, Y) finding the same loss ######\n",
    "# below is just for educational purposes but at scale and everything else, this isn't efficient or fully implemented for everything\n",
    "\n",
    "# This is an example of the transition of before and after the logits.exp()\n",
    "# logits[i] = [-2.0, 0.0, 1.0]\n",
    "# counts[i] = [e^(-2.0), e^(0.0), e^(1.0)] ≈ [0.135, 1.0, 2.718]\n",
    "\n",
    "# Compute probabilities for each class using a simple (non-stable)\n",
    "# softmax: exponentiate logits, then normalize across 27 possible next characters\n",
    "counts = logits.exp()\n",
    "\n",
    "# counts[i] = [0.135, 1.0, 2.718]\n",
    "# sum[i] = 0.135 + 1.0 + 2.718 = 3.853\n",
    "# probs[i] = [0.035, 0.259, 0.706]\n",
    "\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "# get the negative log likelihood and loss\n",
    "# Compute the negative log-likelihood (cross-entropy) loss:\n",
    "# For each example i in the batch (0–31), take the probability that the model\n",
    "# assigned to the correct next character Y[i], i.e. probs[i, Y[i]].\n",
    "# The log makes confident predictions less penalized (log(1) = 0),\n",
    "# the negative sign turns low probabilities into larger penalties,\n",
    "# and .mean() averages the 32 losses into one scalar.\n",
    "#\n",
    "# Essentially: For each example (row) in the batch, look up the probability the\n",
    "# model assigned to the true next character (Y[i]), take its log,\n",
    "# negate it (so higher probs → smaller loss), then average over the batch.\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb012bc0-f5f7-4c2b-be79-3f9b58d59c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_idx</th>\n",
       "      <th>Y_true_index</th>\n",
       "      <th>p_true</th>\n",
       "      <th>-log(p_true)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5213e-14</td>\n",
       "      <td>31.8166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1.2830e-12</td>\n",
       "      <td>27.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1.9647e-08</td>\n",
       "      <td>17.7454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1758e-10</td>\n",
       "      <td>21.8703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6763e-12</td>\n",
       "      <td>25.8947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0823e-10</td>\n",
       "      <td>22.9467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1.8821e-14</td>\n",
       "      <td>31.6038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1.1087e-08</td>\n",
       "      <td>18.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>1.6134e-09</td>\n",
       "      <td>20.2449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2.1917e-03</td>\n",
       "      <td>6.1231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_idx  Y_true_index      p_true  -log(p_true)\n",
       "0            0             5  1.5213e-14       31.8166\n",
       "1            1            13  1.2830e-12       27.3818\n",
       "2            2            13  1.9647e-08       17.7454\n",
       "3            3             1  3.1758e-10       21.8703\n",
       "4            4             0  5.6763e-12       25.8947\n",
       "5            5            15  1.0823e-10       22.9467\n",
       "6            6            12  1.8821e-14       31.6038\n",
       "7            7             9  1.1087e-08       18.3175\n",
       "8            8            22  1.6134e-09       20.2449\n",
       "9            9             9  2.1917e-03        6.1231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss across batch = 17.7697\n"
     ]
    }
   ],
   "source": [
    "p_true = probs[torch.arange(32), Y]\n",
    "log_p_true = p_true.log()\n",
    "neg_log_p_true = -log_p_true\n",
    "\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"example_idx\": torch.arange(32).tolist(),\n",
    "    \"Y_true_index\": Y.tolist(),\n",
    "    \"p_true\": p_true.detach().tolist(),\n",
    "    \"-log(p_true)\": neg_log_p_true.detach().tolist()\n",
    "})\n",
    "display(df.head(10))\n",
    "print(f\"\\nAverage loss across batch = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6aa8b3bc-498f-4959-8d27-808772299186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### In practice, cross_entropy would be used instead - there are many more beefits that are found with cross_entropy instead\n",
    "loss  = F.cross_entropy(logits, Y)\n",
    "# this loss should present the same loss as the calculated loss above\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "21e4ef46-16e4-4936-8211-b5e72e18d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything up til this point has been for the forward pass\n",
    "# This point on is the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "100a3e53-8794-49de-aeb5-7b7c00848ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to ensure that all parameterse has grad set to true\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d868122-d6af-4d55-b1b9-df05480171df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS REALLY BASIC - and this is just an idea / PARTIAL IMPLEMENTATION.\n",
    "## actual implementation will come later\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "loss.backward()\n",
    "# update\n",
    "for p in parameters:\n",
    "    p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b05563f6-4e22-43e6-a821-e1ad9881e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET DATA SET\n",
    "\n",
    "# consider that we only want a block size or starting context to predict each letter\n",
    "blockSize= 3\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    # initialize the context to be of size blockSize and nothing inside\n",
    "    context= [0] * blockSize\n",
    "    # for each character, we want to construct the context used to find answer or predicted character\n",
    "    # we are trying to build something like this\n",
    "    # ['.', '.', '.'] is used to predict ['a']\n",
    "    # and so forth\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        # we add into the context data set used for prediction (in numbers)\n",
    "        X.append(context)\n",
    "        # and then we also add the related answer in integer format\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "        # then we need to consider updating the context to prepare what the context is ued for next prediction\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X= torch.tensor(X)\n",
    "Y= torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7d93e05b-ca75-404a-a47e-18c7a43e902b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FULL REVIEW AND FULL IMPLEMENTATION AGAIN\n",
    "X.shape, Y.shape # dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5f869d70-3588-406b-b7c2-6a5118d29be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is for reproducability\n",
    "# Embedding matrix mapping each of the 27 possible characters to a 2D vector representation\n",
    "C = torch.randn((27,2), generator=g)\n",
    "# Weights and biases for the first linear layer: input of 6 (from 3 chars × 2 embedding dim) → 100 hidden units\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "\n",
    "# Weights and biases for output layer: 100 → 27 (each output logit corresponds to one possible next character).\n",
    "W2= torch.randn((100, 27), generator=g)\n",
    "b2= torch.randn(27, generator=g)\n",
    "\n",
    "# just to organize it all together\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2bbcd7af-2c0f-4974-b327-2cc5d526d291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b99526a6-72a9-4332-a784-642c604de1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters: p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22738cdd-c6f4-4e71-b8b3-ceda72e8dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # 32x100\n",
    "    logits = h @ W2 + b2\n",
    "    # then we normalize and get loss\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f5daa-036b-4255-bf40-5d55cfcb52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f82d71e9-eaf4-4fb7-8e88-3d2b897971c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "83684044-5eb1-4c10-a1dd-770fc429041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "ec5712a2-9d49-4d99-a032-74c90d639ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build a dataset\n",
    "\n",
    "# consider that we only want a block size or starting context to predict each letter\n",
    "blockSize= 5\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    # initialize the context to be of size blockSize and nothing inside\n",
    "    context= [0] * blockSize\n",
    "    # for each character, we want to construct the context used to find answer or predicted character\n",
    "    # we are trying to build something like this\n",
    "    # ['.', '.', '.'] is used to predict ['a']\n",
    "    # and so forth\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        # we add into the context data set used for prediction (in numbers)\n",
    "        X.append(context)\n",
    "        # and then we also add the related answer in integer format\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "        # then we need to consider updating the context to prepare what the context is ued for next prediction\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X= torch.tensor(X)\n",
    "Y= torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "6549b721-f3ae-4b63-9d04-1245973402dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### okay gonna try to move everything to cuda instead since it seems to be killing up my CPU\n",
    "# move inputs to device ( cuda )\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "cc004822-9146-4ebb-b872-bd16946f468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) reproducible RNG + init directly on device\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "# dims\n",
    "vocab_size = 27\n",
    "embed_dim   = 15\n",
    "hidden_dim  = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "f1366cea-5435-40ab-b9d9-5d0d2a2c1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters on device (and with grads)\n",
    "C  = torch.randn((vocab_size, embed_dim), generator=g, device=device, requires_grad=True)       # embedding table\n",
    "W1 = torch.randn((blockSize*embed_dim, hidden_dim), generator=g, device=device, requires_grad=True)     # 6x100 if context=3, embed_dim=2\n",
    "b1 = torch.randn((hidden_dim,), generator=g, device=device, requires_grad=True)\n",
    "W2 = torch.randn((hidden_dim, vocab_size), generator=g, device=device, requires_grad=True)\n",
    "b2 = torch.randn((vocab_size,), generator=g, device=device, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d25e9cf4-f923-469b-99fd-78123d7fa90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 2.5493927001953125\n",
      "tensor(1.2581, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "steps = 85000\n",
    "\n",
    "# STEP 3 let's als include a learning rate exponent (decay)\n",
    "lre = torch.linspace(-3, -0, steps)\n",
    "lrs = 10**lre\n",
    "\n",
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "minloss = 100\n",
    "\n",
    "for i in range(steps):\n",
    "    # STEP2 COME BACK LATER\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # S2\n",
    "    \n",
    "    # forward\n",
    "    # emb = C[X]                          # (batch, context, embed_dim) on GPU\n",
    "    emb = C[X[ix]] # S2 now we are getting a smaller batch from the whole\n",
    "    B, T, E = emb.shape                 # e.g., (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, T*E) @ W1 + b1)  # (32, hidden_dim)\n",
    "    logits = h @ W2 + b2                        # (32, vocab_size)\n",
    "\n",
    "    # cross-entropy expects raw logits and target indices\n",
    "    # loss = F.cross_entropy(logits, Y)\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # S2\n",
    "\n",
    "    # backward\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "    loss.backward()\n",
    "\n",
    "    # update (use no_grad, avoid .data)\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    with torch.no_grad():\n",
    "        for p in parameters:\n",
    "            p -= lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    # lri.append((lre[i]))\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "    minloss = min(minloss, loss)\n",
    "\n",
    "print('Final loss:', loss.item())\n",
    "print(minloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "dc221120-2eb9-4e39-b81b-915b1fa3cf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7e62f4b85f30>]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUE9JREFUeJzt3XdYU9f/B/B3WAFlOgBRcAsq7gluxVVrtcO21hZrrR1f/VY7bEt3tRZba62trVVbtUPE2jq+P+soDrQKKqAoOHALKsPFVBDI/f2BxASybgaXkPfrefI85Obk5pCb3HzuOZ9zjkwQBAFEREREErGTugJERERk2xiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQcpK6AIRQKBa5duwY3NzfIZDKpq0NEREQGEAQBBQUF8PPzg52d9vYPqwhGrl27Bn9/f6mrQUREREbIyMhAs2bNtD5uFcGIm5sbgIp/xt3dXeLaEBERkSHy8/Ph7++v/B3XxiqCkcquGXd3dwYjREREVkZfigUTWImIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFJWsVCepfy8/yKu3L6Dp3r5I8iXC/ARERFJwaZbRv4+fg2rDlxC+s07UleFiIjIZtl0MEJERETSYzBCREREkmIwQkRERJJiMEJERESSYjBCREREkmIwQkRERJJiMEJERESSYjBCREREkhIVjHzyySeQyWRqt6CgIJ3PWb9+PYKCguDs7IxOnTph69atJlWYiIiI6hbRLSMdO3ZEZmam8rZ//36tZePi4jBx4kRMnToVR48exfjx4zF+/HikpqaaVGkiIiKqO0QHIw4ODvD19VXeGjVqpLXs4sWLMWrUKMyePRvt27fH3Llz0b17dyxZssSkShMREVHdIToYOXv2LPz8/NCqVStMmjQJ6enpWsvGx8cjLCxMbdvIkSMRHx8vvqZERERUJ4latbdPnz5YvXo1AgMDkZmZiU8//RQDBgxAamoq3NzcqpXPysqCj4+P2jYfHx9kZWXpfJ2SkhKUlJQo7+fn54upJhEREVkRUcHI6NGjlX937twZffr0QfPmzfHHH39g6tSpZqtUZGQkPv30U7Ptj4iIiGovk4b2enp6ol27djh37pzGx319fZGdna22LTs7G76+vjr3GxERgby8POUtIyPDlGoSERFRLWZSMFJYWIjz58+jSZMmGh8PCQnBrl271LbFxMQgJCRE537lcjnc3d3VbkRERFQ3iQpG3nrrLezduxeXLl1CXFwcHn30Udjb22PixIkAgPDwcERERCjLz5w5E9u3b8fChQtx+vRpfPLJJ0hMTMSMGTPM+18QERGR1RKVM3LlyhVMnDgRN2/eROPGjdG/f38cPHgQjRs3BgCkp6fDzu5BfBMaGoqoqCh88MEHeO+999C2bVts2rQJwcHB5v0viIiIyGqJCkaio6N1Ph4bG1tt24QJEzBhwgRRlappgtQVICIismE2vTaNTCaTugpEREQ2z6aDESIiIpIegxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDEQCCIHUNiIiIbJdNByMyqStAREREth2MEBERkfQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpEwKRubPnw+ZTIZZs2ZpLbN69WrIZDK1m7OzsykvS0RERHWIg7FPTEhIwLJly9C5c2e9Zd3d3ZGWlqa8L5NxvVwiIiKqYFTLSGFhISZNmoQVK1bAy8tLb3mZTAZfX1/lzcfHx5iXJSIiojrIqGBk+vTpGDNmDMLCwgwqX1hYiObNm8Pf3x/jxo3DiRMndJYvKSlBfn6+2o2IiIjqJtHBSHR0NI4cOYLIyEiDygcGBmLlypXYvHkzfv/9dygUCoSGhuLKlStanxMZGQkPDw/lzd/fX2w1iYiIyEqICkYyMjIwc+ZMrFmzxuAk1JCQEISHh6Nr164YNGgQNmzYgMaNG2PZsmVanxMREYG8vDzlLSMjQ0w1iYiIyIqISmBNSkpCTk4OunfvrtxWXl6Offv2YcmSJSgpKYG9vb3OfTg6OqJbt244d+6c1jJyuRxyuVxM1Uwk1OBrERERkSpRwciwYcOQkpKitm3KlCkICgrCO++8ozcQASqCl5SUFDz00EPiamoBHNRDREQkPVHBiJubG4KDg9W21a9fHw0bNlRuDw8PR9OmTZU5JXPmzEHfvn3Rpk0b5ObmYsGCBbh8+TJefPFFM/0LREREZM2MnmdEm/T0dNjZPUhFuX37NqZNm4asrCx4eXmhR48eiIuLQ4cOHcz90kRERGSFTA5GYmNjdd5ftGgRFi1aZOrLEBERUR3FtWmIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEYACILUNSAiIrJdNh2MyCCTugpEREQ2z6aDESIiIpIegxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikhSDESIiIpIUgxEiIiKSFIMRIiIikpRJwcj8+fMhk8kwa9YsneXWr1+PoKAgODs7o1OnTti6daspL0tERER1iNHBSEJCApYtW4bOnTvrLBcXF4eJEydi6tSpOHr0KMaPH4/x48cjNTXV2JcmIiKiOsSoYKSwsBCTJk3CihUr4OXlpbPs4sWLMWrUKMyePRvt27fH3Llz0b17dyxZssSoChMREVHdYlQwMn36dIwZMwZhYWF6y8bHx1crN3LkSMTHx2t9TklJCfLz89VuREREVDc5iH1CdHQ0jhw5goSEBIPKZ2VlwcfHR22bj48PsrKytD4nMjISn376qdiqGU2osVciIiKiqkS1jGRkZGDmzJlYs2YNnJ2dLVUnREREIC8vT3nLyMiwzAvJLLNbIiIiMpyolpGkpCTk5OSge/fuym3l5eXYt28flixZgpKSEtjb26s9x9fXF9nZ2WrbsrOz4evrq/V15HI55HK5mKoRERGRlRLVMjJs2DCkpKQgOTlZeevZsycmTZqE5OTkaoEIAISEhGDXrl1q22JiYhASEmJazYmIiKhOENUy4ubmhuDgYLVt9evXR8OGDZXbw8PD0bRpU0RGRgIAZs6ciUGDBmHhwoUYM2YMoqOjkZiYiOXLl5vpXyAiIiJrZvYZWNPT05GZmam8HxoaiqioKCxfvhxdunTBn3/+iU2bNlULaoiIiMg2iR5NU1VsbKzO+wAwYcIETJgwwdSXIiIiojqIa9MQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMABAEqWtARERku2w6GJFJXQEiIiKy7WCEiIiIpMdghIiIiCTFYISIiIgkxWCEiIiIJMVghIiIiCTFYISIiIgkxWCEiIiIJMVghIiIiCTFYISIiIgkxWCEiIiIJMVghIiIiCTFYISIiIgkxWCEiIiIJMVghIiIiCTFYIQk883OM3jp10SUKwSpq0JERBJykLoCZLu+2XkWALD/3A0MatdY4toQEZFU2DJCkispLZe6CkREJCFRwcjSpUvRuXNnuLu7w93dHSEhIdi2bZvW8qtXr4ZMJlO7OTs7m1xpIiIiqjtEddM0a9YM8+fPR9u2bSEIAn755ReMGzcOR48eRceOHTU+x93dHWlpacr7MpnMtBpbgADmLBAREUlFVDAyduxYtfvz5s3D0qVLcfDgQa3BiEwmg6+vr/E1tKBaGBcRERHZHKNzRsrLyxEdHY2ioiKEhIRoLVdYWIjmzZvD398f48aNw4kTJ/Tuu6SkBPn5+Wo3IiIiqptEByMpKSlwdXWFXC7HK6+8go0bN6JDhw4aywYGBmLlypXYvHkzfv/9dygUCoSGhuLKlSs6XyMyMhIeHh7Km7+/v9hqEhERkZUQHYwEBgYiOTkZhw4dwquvvorJkyfj5MmTGsuGhIQgPDwcXbt2xaBBg7BhwwY0btwYy5Yt0/kaERERyMvLU94yMjLEVpOIiIishOh5RpycnNCmTRsAQI8ePZCQkIDFixfrDTAAwNHREd26dcO5c+d0lpPL5ZDL5WKrRkRERFbI5HlGFAoFSkpKDCpbXl6OlJQUNGnSxNSXJSIiojpCVMtIREQERo8ejYCAABQUFCAqKgqxsbHYsWMHACA8PBxNmzZFZGQkAGDOnDno27cv2rRpg9zcXCxYsACXL1/Giy++aP7/hKwWB1YTEdk2UcFITk4OwsPDkZmZCQ8PD3Tu3Bk7duzA8OHDAQDp6emws3vQ2HL79m1MmzYNWVlZ8PLyQo8ePRAXF6c14ZWIiIhsj6hg5Oeff9b5eGxsrNr9RYsWYdGiRaIrRURERLaDa9MQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjBAREZGkGIwQERGRpBiMEBERkaQYjJDkBM4HT0Rk0xiMEBERkaQYjIBX5kRERFKy6WBEBpnUVSAiIrJ5Nh2MEBERkfQYjABIvZondRWIiIhslk0HIwmXbgEAlu27IHFNiIiIxLmWexdjvv0XfyZdkboqJrPpYKRMwcxVorqkuLQcL/6SgOjD6VJXhQwgcPSASeb830mcuJaPt9Yfk7oqJrPpYISI6pbf4i9j56kcvLshReqqkB4Zt+6gz+e78EPsOamrYrWK7pVJXQWzYTBCRHVGfnGp1FUgA325Iw05BSX4cnua1FWhWoDBCNUCbKolsjUKlS6a20X38OX20ziXUyhhjUhKDEaMdK9Mgbv3yqWuBhGR1Xt/Uwp+iD2PUd/sk7oqNSK/uBQ/7j2PjFt3pK5KrcFgxEh9I3eh/UfbUVzKgKQ2OJNdgMeXxmH/2RtSV0VN/PmbOJtdIHU1qA44m12ApMu3RT/v5LV87Dtz3QI1Mp+j6bkAbGdQwcebT2D+ttMYu2S/1FWpNRiMGOlW0T0AwIXrRRLXhADgpV8TkXT5Np79+ZDUVVG6eKMIE1ccxPBFtnG1R5Y1fNE+PL40Djn5xaKe99C3/yJ85WFcvMFzVW0Rd77ioin3DnOcKjEYue+rHUyismY37weHtcl59n+TBVzJvWvU8y7dZDBCtReDkfuW7OHwMiKimsKVwUgVg5FaJiuvmHkoVKNKyxV4ZsVBLPyHrYPWwOh5wmwjHaPW25OWg+z8ElHPOZtdgPSbd/BHYgYK6ujwdQepK0APnMspQNjX+9DMywX73xlq0HM2Hb2Ke+UKPNnT38K1q914lWW8HSeyEHf+JuLO38SbIwKlrg5RnTZlVYKo8mlZBRipMsoo5mQ2VoT3NHe1JMdgpBbZcSIbAHDltmF9wsWl5Zi1LhkAENbeBw3qO1mqalSH3StTSF0FEoVNHLbkwDn1EYIxJyt+J4pKyvBvLRs9aAp201gx1WFwd+rQtMDGkMnYNkJEtiM6IUPqKpgVgxELOZ2Vj2dWHETS5Vs6ywmCgDwO7zLItdy72HEiy6jFtY6m38bqAxe5MBfhi+2nsXzfeamrUeMEtqjUKaXldatFk8GIhUxeeRhx52/i8aXxOsu99FsSusz5B8ev5KptFwQBS2PPV2uiq4sMjQ9C5+/Gy78l4X/HrlV7TF/DyKM/xOGT/zuJrSlZRtRQu7p2QjCHs9kFmL/tNHLv1L7h1pduFGFp7Hl8vvW01FUxWl2Jp22xNTMzz7hh2caaFX0Uz/50yCouwhiMaHEup9CgHxptVxuGZktX9v+tPnBJbfvOUzn4YvtpTPqp9kziVVvEn79p9HPNufbF2sPpaPv+NmxPNW+AY+2GL9qHH/eex4ebT0hdlWruWOESDuevF+LX+EtSV8OibCUsuVlovgDdkPhiU/I17D93A2etYM4jBiMabDhyBWFf78WLvyQaVP7SjSL8e9b06ZZVo9d0kWsWCALw2tqj+Ghzqsn1sEZSnMwi7i9T/8rvSRK8umWtT8xAv/m7cSoz3+h9pFRp7SPjDFu4Fx+ZIbCzgotjshBrOPaigpGlS5eic+fOcHd3h7u7O0JCQrBt2zadz1m/fj2CgoLg7OyMTp06YevWrSZVuCasPHARALC3ynoO13Lv4vDF6jkgg7+KxXM/H8b21MwaqZ8mF28U4X/HruHX+MuS1aGmlZSV43pBRQtUbWryLS1XKJcLsFaz/zyOq7l3MSPqCH6Lv4TLtWj2zhuFJbhm5CykdYEV/K4QiSYqGGnWrBnmz5+PpKQkJCYmYujQoRg3bhxOnNActcfFxWHixImYOnUqjh49ivHjx2P8+PFITbXOq/fQ+bvx5LJ4JGfkanz8ld+PGL3vqicYbX18uXfuKVd6VP35tbbcBU3/3z8nsvDPCcO7PIZ+tRe95u1E+k3drUiGtnCZy+jF/6L73BhcuW2eFTmz8opxVaIf3/PXi/Dh5hMYtCBWktfXpOdnOxE6fzfya+HkTwqFgC3Hr3E1VjIbbddZdS0hWVQwMnbsWDz00ENo27Yt2rVrh3nz5sHV1RUHDx7UWH7x4sUYNWoUZs+ejfbt22Pu3Lno3r07lixZYpbKS+WIjpUzTRkZY0hTWtc5MRjw5R7Ri2VVOnThJsJXHsalG0UoLi3H3jPXUVJW8/3oC/85o3a/qKQML/2WhJd+S9I7TLnyy1n5A737dLbWssWl5dh5SvvjllCZlxJjhtctVwjoG7kL/ebvrvXDt89fL8QfiQ+GG1r6VHnlVu1rHdmUfBUzoo5iwJd7auw1j2Xk4svtp3FXRD5MVl6xqPIXbxQhNi3HmOppVXvaMs0rM+8u9p65bnDSaFm5AnvSckQH19bQ9SKG0Tkj5eXliI6ORlFREUJCQjSWiY+PR1hYmNq2kSNHIj5e9wiTkpIS5Ofnq91qk0Qdw3W7zPkH21I0d9coFILRQQSg3vpx4pr6e2LoB/Op5Qex78x1zFh7BG/+cQyTVx7GJ/+r+UTDqmsB3VWZAr+49MH/KQgCykxo9an6vvyRmGHRH/X9Zp6ESHVCshsFNdP1s2LfBaOeN2zhXrz953Ez18ZwOQXFKC3X/UWwdG+eOXI79Kn6mR73/QH8EHse3+0+a9DzLt8sqghwv9ht8GsO+SoWz69KwJF07RdiVCEkcjcmrzyM3aerB2+aztPf7zmPKasS8KwFByvUol5srUQHIykpKXB1dYVcLscrr7yCjRs3okOHDhrLZmVlwcfHR22bj48PsrJ0N8VHRkbCw8NDefP3r11TnesbHvrqGs3dNbPWJaP357u0dkXoiyfCvt5rSPW0Uo3Us/KK8ff9oGnt4do7ec7UXxLR5/Nd+ltLDNzf1dy7mLvlpOkV0+LZny13Qhm4YA9OXMuz2P4rzdt6yuCyG45cwX/WJBm9nlJZuQJ/JGSYnJOSllWA3vN24ce90s4fUlgiXevVmWzDRkxU5sIZk9eUetXyn7+6wtBRf38duQIAOH7Ftt9b0cFIYGAgkpOTcejQIbz66quYPHkyTp4078k9IiICeXl5yltGRu39sTTU3Xvlyvkxvo45U+3xqk16W45Xb125rJIbYUx/oaZ9ViorV2D/2RuiT6bFpeWYHnUEfyZdEV0fQ+w+nYObRfewqsrQZ1VpBp6EK1VOu29pB84ZPwT5THYBHvvhQLVRWjOjk02slXm98ccxbE3J0nl8dPn94GW8/ddxk3NS/nfsqknPtyY1NWdE+s07mLLqMA5e0P85VigEnMkugEJhXN10Pe3PpCtaW5tJnMKSMrzyWxL+T8NcTVITHYw4OTmhTZs26NGjByIjI9GlSxcsXrxYY1lfX19kZ6uf+LOzs+Hr66vzNeRyuXLETuXNmt0quodFOx8EIKezCvBOlebsqt9FbUmyyvJVnmDIKSBOR6T+/Z7zePbnQ3h+5WED9vRA1KF0/H08E2+tPybqebpoOtku2KG6oqx6O8jaw+m4qeUqz1xJXuk372Dsd/ux5bjpX2KFQsDfxzN1Jri+8lsSjqTn4qXf1IcNm9JlZUm5d43rQkq4ZHyzf11L4LM0fe9WWbkCt1W+R69FH8WetOt4ernmnEAAyLh1B3Hnb+CL7acxYtE+fC6iVU1Vlpbu6+z8Yry1/pjW1mZLulemwJH02ygXEWAZc1GmrwvF2LlxNJ1Hl+09j+0nsvDftUeN2qclmTzPiEKhQEmJ5gm+QkJCsGvXLrVtMTExWnNM6qruc2OwvEo//LpE01p7pv6SiJMqc0CIvVrSlEsBAIk6knNV5d65h+LSclGzbJaWKzDu+wOI2FA9EJNVuV+b7DqVjYEL9iDlah5mRFV8iY3tlgCADUevYnrUEfT/QnuS441C3ZPmZecXY/qaIzhkwFWrVKROsCspKxc9ykyhELA9NQvnrxdizLf/InKbcT+uAPDNzjNWtdTD2CUH0G1ujLLLzJDZQgd8uQfPrDiEZffPbz/tv4j84lKztd7k3TX9/VMoBLz8WyIm/BiHN/84ppwOQJ/X1yXjsR/i8M3O6i3ZmiReuqV2UabpHRAbQH+25WSVCzGVfWl4j++VKZByJa/aY1GH0vHSr4m4lvsg6Jv3t+W6q40hKhiJiIjAvn37cOnSJaSkpCAiIgKxsbGYNGkSACA8PBwRERHK8jNnzsT27duxcOFCnD59Gp988gkSExMxY8YM8/4XFqRvRdPfD6YbvW9Tv7AzTYhutbUkGCLvTim6zolB97kxop534NwNHMvI1ZijUnWekG93ncXX/2j+EtakK7fvYGqVocGl5Qq1Jb3FijtvepLrO38dx98pmXhK5ar1nxNZiDqk/fNYXFqOv5KuIKegGOUKAVGH0nEup8DkuuhSVq7AuoR0XLxh+jwlH25KxchFhr/vXT+NQUjkblHfs83HruKV35MwbOFenLiWj2V7jUvmBYBvdp7FhxaYhNBSMV7lBHfbTJxRuPMn/+D9Tcb/3yv3X0SRiO7i19YexZM/xmvtIkq+kosdJ7KRcOk2/jpyBe9tTDFov5U5dSv+NewzcMGAz7i+j2JOfjE+2JSC01kVx+Kn/RcNeu1K/1mThLFL9uOnfy+qzYm1Ou4S/jmZrcxPAYAV/15EVp7xAyrMzUFM4ZycHISHhyMzMxMeHh7o3LkzduzYgeHDhwMA0tPTYWf3IL4JDQ1FVFQUPvjgA7z33nto27YtNm3ahODgYPP+F2ZSVFKG+nL1tyRiQwq6BXhqvcJae9j4YER1ynhdH9K798o1XilfM/CDtP/sDTRyczK4Xi3e/Ru/vtAbc7acxDujgjC8g3oScsr9JLY798pFnRgN/U24e69cY15NpY1Hr+DzR43/DIlJ3NPUehH4wTadfdw1QdMMvZVdOn1bNUCrxq7VHl+08wyW7b0APw9nzBjaVnlS/vrJLqZXSMv78fvBy/jk/yquwC7NH6Pcvv/sDbT2ri/qJX47KG5Cv7ul5bhbWo4yhQBHe8NSnE3J89EkycCWRjGMvYapyfVJog6l4/NHO+ksc1LL7L5ztpzE2ZxCRD6m+/nlCgH2djJlLt7JzHwEN/WoVq60ygWluUe86XI19y6mrk7AlH4t8FSvAI1lVD+Zs9YlI+78Tfx+MF3t+6JJ1cP55fbT2HmqYgTPqgMXDfp9qE3zU4kKRn7++Wedj8fGxlbbNmHCBEyYMEFUpaRyvaAE9ZzskXr1wZfkryNX1KJJc1JU+TRpO1cMWrAHOQY2LVZ14XqhcoTHxN6avwwVr63+4uH3c0em/ZqIRU91Qd9WDVGuENDMq57O11uXkI7ohAysCO+JRq5yg+pYUGV8vb4ukOJSBULn6x6WWFauwLrEDHRp5mlQHcSQOhABgAvXtV+F3Sq6h1aNq2/feX8dpGt5xTimJyfJXDR1++09cx2T73++xnRqUiP1kEpNTlZn7uGbhq6vZSxdP4RrD6cj8rFOajksgiAoW1Bz8osxbOFejOvmp/K4Ya97t7Qc+cWlcHd2NK7iKsoVAm7fuafxXJeVV4zPtpysyBH8K0VrMKKq6pQNYvwQa/xIsv1nb+BuaXm1C8+axLVpVIR9vdfgJjlL0NafaEggovpM1fVETmWa3gz/+rpjCIncjf5f7Kk2xLbqCeCdv1JwND0XPT/bqbb9aPptrUmO7/yl3mz61xH9IyMy9UT9v8ZfxvsbU/Hwd/v17stcTMkjMVRpuaC369AUfyQYl8u0TMPcJNo+z4aMzqjKEldwNdVSYO7hvpXvqyAI2CDiQulMtmW75Czhs7815+ysPHAJBSVlRneTi5kwT6bSdiEIAi7eKFJ+dp5ZcRA9P9tZbdV1oKKbp+raTprO5cYuZaHr05tfbPhnTqEQ8OzPhzDt10SD82ksgcGIijKFINnS4hm37+CbnbonLdJF9byq2qUzPcq8Weg3C++pXYHpSsiqDFz2nbmOR3+Iw+vrtI+4Uf063jZxXZdTmfkmN4+fuJaHAV8aPikUACw0U45L3t1SrSeTq7l30cPAXJ0ZUUcwZdVhUT+6b/+le9KywpIyZJswcZ+xPtMwN8y9MgUmLj+I73bp/95sT83EmkOX1T67/zt2DY/+cEBtnRtTGhe0ddku1zD3ibYRGnvScrA09jw2J1/F6MX/4pKOPIQ9aTl44w/DR7F99Y9hiZi63gNzxm/63uv0m3eQYaYlFaoSk3ivOiHj1zFnMOSrWHx5P6n00P28jLWH0zX+P5eqLFUx7VfDl6bYeFR3oKkryV1MAKx6SPOMHBVnDgxGaomj6blm32fVqyZTf+Q1ycp78IXYdFS9RaPyfLvLgGnRzdnE/PJv+lfR1TdS4L9RR5EhcrpxQ+cvkWk4bd0rUyiDhocW/6vz+QV6TjTC/f1tOZ6JPWnXMXlVAqasOozzKl075Ub+qgR/vAN9Pt9lcECi6apPpvWOdr9oWAByc/I1xF+4iYU68ouAih/QV34/gvc3puKEShfszOhkHE3PNdsMxJWrOFdVNbBMv3kHwR/vQKTKMNi48zfuz+uRgC+2n8bM6GScyszHuxuqB4c3C+9BEASc1NCkr1AIetdEknqUUyV9LQKz1llu+OkzImc7vVlYAkEQ8N3uipmjl1bpEjHlPdX2Lui6eAOqd/NbOwYjErLUFL2Vn9GqV026mroN/VhXrrtSSTWfZta6ZLXHdp/OwVEDp4/+zYyrDRcUl6JET1fGhB81L0kgCAKKS8uNztExxvWCEnT4aDtevb/Qotg8g7PZBWqzPZ7PKVQLtvaduY49aeqTp201YhIp1dk3da3PpItCIajl3Kguj5CZdxcL/0kzOMNfdU2l7/cY1l/+pob5cFSvImti2uxvdp7B3dJyZdfW8Su5eGbFIQxcUD1ZWtMcE/9dexTztHRfvP5HMvp/sQebkysuDMwxNNYSDpy7oXeE1fXCEhSoBHKCAPx79jpOXssXNUTWHBMF5t4trTYySt8CnSSOqARWqr2W7XtwMrZU0yYAvPBLAtZM7WNQ2dfuDz2eHNJcb1l9V7di3L5TqndxvCu3q//gKxQCAj/cpnd9E3PbcOQKyhQCtp/IQpKOdY+0Gb5oH5zsH1xXvKvlCl2VMRMpLVXpbnh1zRHsmDVQ73NUf9tbvPs3POs5Ildl7g3VSc+mrErA6awC7DqVg60zB+jdd5mBx+nX+Et6yxSXlqPP57s0/ngv3nkWLRvXxyNd/DQ803THjJgG/Kf9F2Fvpx45yVDRWgQA3+85h8HtvNFt7j/mqKJGVRPPxZhkYMuEanfWxZtFeO7nisTnlwe10vqcnPxiZOUXo0MTdzjY22mdUE2sqvkpY77V3YJpMCMDYEs0jEjZ2MJgREKlZeY78qrdPO9vTMWkPvoDAGPUsZZBNVGH02s8EKnq8aW6F5HU5p4JCZ6G5h38XWU5AX1zrWj6rOTqmATsdFZFgqW2IZ9VrTdwtkttSZCVruXexc/7L2ptRaicPVk1GLl7rxzrEtJRWFIGP08XPNa9mdb9r0vIwPtj2sPR3g7lCkH5f1b6TUewdENHC13VvJN/Tj4IwM9kF6LLHMMDEWNah4Z/bdicLyVl5cjKK0bzhuKGclftJr2oYwQZUBF0urs44uf7c3N0buaB5/pa5jwIqHeXCoLxiaiqxOR3metMteaQ+VqlTcFgRELrEo2fo0SfDA3zUOhiySBDU39/bRJ9P/HwAxMnatI090elb3edRWjrhujZooHFJxojcS7dvKN1lkttvtxxWm09nmHttQ+JvFtajtUHLmHawFZ4+8/j1YItXQvcGTqXkKli06qvMKvJ+qQMTA5tAUD7FO5Vjf8+Dqcy8xH1Yh+EtmlkbBUNqJt6cHr8Sh5mG7CK9N175XBxstdZRtdSGoBhM6tqSyq9a+R07+aQU1CsttK0lKv7MhiRkKH93MYY8GX1/mddy3+LacpcaeSiaLVR+s07BnVr6DNHz0rAX8ecwdcxwM43BhnVLG+trGHpcmNUXZG1RM+w7nlbT+Foxm29K37XtOlrjuCdUUEGX4yozsFkqMrhrX8euYILN4qMXlTzRZWRKNfNNAfKiWt5GPPtfkzs7Y/Ixzpj09GraNGoPrr6e6qVizmpu9tXEKB31tgPtMz8qm+aAk1SruTpnG3ZUAsNHGFVExiM2JDbZlonQ18+hrW4eKOo2qq4lpZWpYl+2q+J6BbgWaN1qClXbt/VmJtjiE1HryKsgw9c5bXnFFVcWo4V+y4goGH1if8MCeZrQyDy7E+HMKy9t/L+3ymZ+DslE+18HszYW1KqMCkfRBdTWh9V6QsODPX9norRMWsPZ2BER19lEr6+2U+rOnDuBjr66V7QdauJ0+yrGrvEPPMnVW3xYc4IkQSGfBVb469ZOcqhUszJ7GoTyVHFyKwxnZrg+0ndpa6K0uJdZ6sN6az0yJIDNVwb4+w/dwON3arPFqraVbQw5oxZE8ormXPWX33D240xZVWC8u+qXan7zui+aKmp7rS6jEN7qdapy0vD/6Phis7c66HUFX8bMfzYkrQFItbGHMGvMbPXnteTgFrThn+9t1oycaUwA5NzVelbJsKSMyebC3NGiFRsqwXN2UQ1QdtKs5Zk6OR8umgKqqsqKSvH7lOGJcZK4WyO9sRhYyyyQGuSLWEwQrWOJZpgiWojKdfCMtbNwhJ8a8AU/F9sS8PKAxdroEa1gznOW3W3TVg/dtMQUa1Vk8veS8Eaf6xfWJ2gvxCs83+TWoGIBe4sgQmsREQa9Jq302IT+NUG2WYaolqTbGloOtUctowQUa11o/AeFhvQJUBEppMygZXBCBEREUmKwQgRERFJmjPCYISIiIgkxWCEiIiImDNCREREtovBCBERETFnhIiIiGwXgxEiIiKSFIMRIiIiYgIrERERSYs5I0RERGSzGIwQERGRpBiMEBEREXNGiIiISFrMGSEiIiKbxWCEiIiIkJyRK9lriwpGIiMj0atXL7i5ucHb2xvjx49HWlqazuesXr0aMplM7ebs7GxSpYmIiMi8JOylEReM7N27F9OnT8fBgwcRExOD0tJSjBgxAkVFRTqf5+7ujszMTOXt8uXLJlWaiIiIzEvC/FU4iCm8fft2tfurV6+Gt7c3kpKSMHDgQK3Pk8lk8PX1Na6GREREZHFW0zJSVV5eHgCgQYMGOssVFhaiefPm8Pf3x7hx43DixAmd5UtKSpCfn692IyIiorrJ6GBEoVBg1qxZ6NevH4KDg7WWCwwMxMqVK7F582b8/vvvUCgUCA0NxZUrV7Q+JzIyEh4eHsqbv7+/sdUkIiIiA0jZTSMTBONGFr/66qvYtm0b9u/fj2bNmhn8vNLSUrRv3x4TJ07E3LlzNZYpKSlBSUmJ8n5+fj78/f2Rl5cHd3d3Y6qrUYt3/zbbvoiIiKzZVxO64Ikehv+eGyI/Px8eHh56f79F5YxUmjFjBrZs2YJ9+/aJCkQAwNHREd26dcO5c+e0lpHL5ZDL5cZUjYiIiKyMqG4aQRAwY8YMbNy4Ebt370bLli1Fv2B5eTlSUlLQpEkT0c8lIiKiukdUy8j06dMRFRWFzZs3w83NDVlZWQAADw8PuLi4AADCw8PRtGlTREZGAgDmzJmDvn37ok2bNsjNzcWCBQtw+fJlvPjii2b+V4iIiMgaiQpGli5dCgAYPHiw2vZVq1bh+eefBwCkp6fDzu5Bg8vt27cxbdo0ZGVlwcvLCz169EBcXBw6dOhgWs2JiIjIbKxmnhFDcl1jY2PV7i9atAiLFi0SVSkiIiKyHVybhoiIiCTFYISIiIgkxWCEiIiIIJMwaYTBCBERETEYISIiItvFYISIiIgkxWCEiIiIJMVghIiIiCCTcNozBiNEREQkKQYjREREJCkGI0RERCQpBiNERETEeUaIiIjIdjEYISIiIkkxGCEiIiJJMRghIiIiSTEYISIiIsgkzGBlMEJERESSYjBCREREkmIwQkRERJJiMEJEREQSLpPHYISIiIjAGViJiIjIhjEYISIiIkkxGCEiIiJJMRghIiIiyCRMYWUwQkRERJJiMEJERESSYjBCREREkmIwQkRERJxnRCq9WzaQugpERES1AmdglUgjVyepq0BERGTzRAUjkZGR6NWrF9zc3ODt7Y3x48cjLS1N7/PWr1+PoKAgODs7o1OnTti6davRFSYiIqK6RVQwsnfvXkyfPh0HDx5ETEwMSktLMWLECBQVFWl9TlxcHCZOnIipU6fi6NGjGD9+PMaPH4/U1FSTK28qKcdUExERUQWZIAiCsU++fv06vL29sXfvXgwcOFBjmaeeegpFRUXYsmWLclvfvn3RtWtX/Pjjjwa9Tn5+Pjw8PJCXlwd3d3djq1vNf9YkYWtKltn2R0REZK1+fLY7RgU3Mes+Df39NilnJC8vDwDQoIH2RND4+HiEhYWpbRs5ciTi4+O1PqekpAT5+flqNyIiIqqbjA5GFAoFZs2ahX79+iE4OFhruaysLPj4+Kht8/HxQVaW9haJyMhIeHh4KG/+/v7GVlMndtMQERFJz+hgZPr06UhNTUV0dLQ56wMAiIiIQF5envKWkZFh9tcAIO04JiIiIgIAOBjzpBkzZmDLli3Yt28fmjVrprOsr68vsrOz1bZlZ2fD19dX63PkcjnkcrkxVSMiIiKjWMlCeYIgYMaMGdi4cSN2796Nli1b6n1OSEgIdu3apbYtJiYGISEh4mpKREREFiPlDKyiWkamT5+OqKgobN68GW5ubsq8Dw8PD7i4uAAAwsPD0bRpU0RGRgIAZs6ciUGDBmHhwoUYM2YMoqOjkZiYiOXLl5v5XxGPvTRERETSE9UysnTpUuTl5WHw4MFo0qSJ8rZu3TplmfT0dGRmZirvh4aGIioqCsuXL0eXLl3w559/YtOmTTqTXmuKTMowkIiIiACIbBkxZEqS2NjYatsmTJiACRMmiHmpGmHCFCtERERkJja9Ng0RERFV4EJ5EmE3DRERkfRsOhghIiIi6dl0MMJ2ESIiIunZdDBCREREFaRMXbDpYIQpI0RERBWYwEpEREQ2y6aDkTaNXaWuAhERkc2z6WDksR66F/kjIiIiy7PpYKSppwtmDmsrdTWIiIhsmk0HIwDg5vxgRvyHOvlKWBMiIiLbZPPBiOpQJhlnHiEiIqpxNh+MEBERkbQYjKgYEuQtdRWIiIhsjs0HI34ezsq/XRzt8derIRLWhoiISBpSTgRq88HIyI7qSat2nJaViIioRtl8MGJnJ0PrxvUBAKGtG0pcGyIiItvjoL9I3bdj1kCUlClQX+6Ay7fuSF0dIiIim2LzLSMA4GBvh/ryirjMjr00RFbt/YfaS10FIhKJwUgVHf08pK6CaPMf6yR1FYhqjcmhLaSuAhGJxGCkCnsLN43EvjXY7Pt8uneA2fdJZK2Yg05kfZgzUoNOfDpS2R1kKxY/3RUymQyvrT0qdVWIiKiWYsuISE4Oxr9lmgKRIF+3atuaN6yH9a9onu/kjeHtjH59KbTxdsUjXfxwaf4YqatismMfj5C6CkREdRKDEQO18XZFQIN6iHqxj1n3u33WQBz5cDh6t2ig3Lb1tQHopXJf1WtWtspwXVnv5/NHO8HDxVHqahCRDXu0W1OL7p+TnlmBBU90xr63h6CnhiAhpNWD+UmaqMzoaqgG9Z0wtquf8n5lC8rHYzsYUVP9jKljXTM4sLHBZRM/CMMzfZiXQ7XPhw9b5hyhC+djks7DnZuYfZ8rwnuafZ/GYDBioG4BXlof++rJLtg2cwDef6g9wkNaKLcveKIzOvq5G/2aU/q11FtGTNfB9CGt0dTTBT8+20O57YvHTRuJ85ieSL22JhN+/0x3g8r1a9MQjVzlyvu9Wmj+HPg3cNG4vZ6TvfjKwfTjUtO0vS+WsnpKL62POZgxCb19E+O/v9q8acau1qn99Z8jzC1qWt8af82mni74bHxwjb+uLRjewUfqKgBgMKJTPSd7tGxUH/++PURt+7Lneqjdb+rpgvZN3DFtYCu1E+GEnv4ap5d3sjff2y6m6+Dhzn448O5QdPH3VG5r7CbX+Lehpg7QfTKUIhjRFhioqi93wPIqx1GTvi3VrwJXT+mtsdzHD3fE7jcHVduuLfdHnyd7+osq/8Mkw4IrS/l4bEeL7PejhzugZ/Pqgc7gQM2LWjas7wSZTIbED8IwvIOPyVd9fxp5/HRpqBLcdg/wNPv+6yJvdzme7dscO9+o/h2TSudmNT8NhCAYVk71AkqXR7r46S9UQxiM6PBkT3/seWsw/BvUU9s+sqMvJtVQs/1vUx/8+B14dygAIO2zUYh8rBMORgwTtS9NgYHqh9vQD7qqNt6u4p9kYQ3qOandd3N2gItj9RaKEVXWJTJEfbkDnB01f23qOVVPUO7o52HwlfquNwdh1ZReiH6pL2Qio7gRIq5uKk+ihp6wpPRC/5Z4UU/Aq0kjVzlWhPc0+Kpvw39CNW6vL3fAsud6wMXRHp8+0hFd/T0xsJ3hXXz6qLakWpo5unTkJiTwG0Lfx75xDX5mP39Ud+uko46LyqdEXkyI0dRT/8XWRi2fZ1Vpn43C4qe7AqhIFQCAbv4128KpisGIDu46Wh20fRAFGPGLDmhN8xzQtjEuzR+Di5EPKT+Ecgd7TOwdAF8duR/DgjRfORrq6ye7KP/W9aMld7DHTAOTaj99pCMGtG2k/AJYipuz+nFbO60v9s4ejCXPdMPE3gE6m/gtIfGDML0/BE09XdC6sSuGBHqjbytxffKPd28GBxGtbcue64EX+rXEX6+G4PD74gJabZp56T9BGk/3L9S4rrqv7kZ21B+QdNfRDTuyoy9OfDoSk0NbYNP0fvj1Bc2tY4bqqtIyqenKdLye/8cQQb5u2PLf/nhrRDscjBiG+IihZunSqex2XPeSZbpqEt8PU/69b/YQHSUtr9X9NcuM8cUTnfWWMaSFfEQHH/zxsnrrnKO9/gsV/wb18NrQNjrLyB3slRc98RFDceyjEfCq76TzOZbEYESDr5/sgmFB3nhpYCutZQRjmhFMIPZKeUV4Txz9cLjRr/dY92bKv3+e3BPHPhqBF7WczHT1q6uOppkc2gK/Te0DuYNxeRSVIvXMONunpXqScXBTD3i7O+Phzn6IfKyT1ib+J3o0U+vC0kbbCCFvLd1cnvWcMLV/S3jWUw+S3J0ftKS46ph/5qsJXbQ+tnRSdyx8UvvjmjTxcMFHYzugecP68HYzPpm5q78n5ozriF1vDoJnPadq/980I1o0Kr01op3BQevoYN0tXPU1tFiJZVeldUs1V0pTqxsATNQyGWEHP3ds+E8o4t4dCjs7GY5+OBwJKj/CbX2qD/fXZeXzPau1jG2fNRDBTT0wY2hb+Ho4o4mHeYLFyvNQH5EBszHqyx+8r5XvvrEXe5ag7TfA0OR4Q/6X5eE90VvlfKbrGT8+q95V26mZp0H1ACoCE4960o4WZDCiwWPdm+Hn53vp/IEwlKYYokWjetU3mpmdncxsUa6dTAaPeo54e1SQ2vb3Hqq4r+vKU18M1UCljp8+op53oCmyDw9pjlF6uldkMsDHXXxz7tO9/PGLCa0mdnYynV0yDnbav25NLdqyYJyuegKzTdP7ITykBVo3ruiqU23CD2hQD++PMb5bYMbQthjX1bLDGKuKeX2gwWVVZ2r+UstV8Oth1VsMK1sRuwd4we9+S6dXfSej8rUqDQ3ywXKV3Jia6n6LfKwT/DycDQrgDSX2osuSDKlJ84bVz+W6vufm8O7oivPu5JDmatu93XVfWJybN1qZf/XyIO0X2lJhMGJmVbsINFkR3hMPdfLF/2b0q4Ea6abpy6RN1QnfXhrYGkDFCeTXF3rjjeHtql0Nijm1VF1TZGZY9VEHc8apZ9SbO2fFs55xAVxll5lq117VOWnC2qu3yKieeHW19phyev7fjH7wM3IotyWWRjg9d5To57Tx1t1crqvvXoy2Pm5GDVvV9PvZ1ttV44+Dn6flh9WHGPg/qF5xG5KHUNXE3gGIixiGtmb8DnrVc8SQwMYYHNhY7ULFnDRNNKlNaz1dNf8dWj3gNDSeUm1h1dR13MhV8/8/KrgJjnw4HJ9UuXjrHuCFD8a015i0veW//eFgb4dVU3ph5fM98ebwQMMqWYNEf4v37duHsWPHws/PDzKZDJs2bdJZPjY2FjKZrNotKyvL2DrXCtqayx7v3gwPdfLFvEe1D0Nr3rA+fpjUA51FNKOZ25b/9seq53uhjfeDL6a2xEzVL1fl1W9AlaTege0a47VhbfH+mPZ4XUMQYQx7OxkWPNEZnz7SEVv+2x/736neh7xWyzBDYyZb03RMm2kYmaP6fni4OOKz8cEIblqRFKoacIS2aaT2PAcdfb0+eq5qtKmaXF1V52aeiIsYhgFtG2ktoy1H4dVBrY2qky7OGro0Vj7fE8c+HqEcBdWwyo9QG283rJ7Sq9r8OK8NbXP/h0t3fpSYhv2hJuZaVWrRqPqP2Liufmrdn4ZK/XQkhoiYF8dQ41VanqJf6qu1G1YfY3usn+5VPclTJpNh1ZTeWD2lN2QymTIfaMb9VlJzLKfxh44RUr2rzCO1dlpfhFdpgVA1tksThLRqaPI5r+pneGiQN/a9Xf18V9k11OD+qLGqXhzQSpm03UVltE/l+cnN2RFDg3xMmkncUkTXqKioCF26dMH3338v6nlpaWnIzMxU3ry9zfOlr22cHOzww6QemNRH+wdYk373f7jMOey3KtUf6OCmHhhy/8T7xvB26NHcCxteDUUzLxdM6dcCADC2ix96tfBCB5WckE3T++Hhzk20JoG6yh3w2jDdiVNiTOjpj8mhLRDc1APNvCp+eFX77zVdvJuzeX9cl+r7mj7kwf+37uW+eLbvg2NtidaEjk0fvP9VW1cqTzKqWjWuj79eDVHLQ/DTkTOw6KmuavlFbbxdcfi9YQirgfkHGrvJMTTIBx4ujvjthT6Y0KMZ1r1c/cdicKB3tSvaN0YEYtWU3mrvuaFXpW5aftSeN3DFX32/v1WTukcH+2Lx092MasVxlTtg1ZTeehMSxXh1cGuM7dIE7s4OGBbkDf8G9fCBAaNtzPnp1jdaBQC+eaorEj8Iw9Cgis+io70dDkYMqzaSsI23q8EJp+46Wq/Xvax+cePt7ow3R2huRfBwcYTcwR5rX+qLmRq65PQZdT/XqYOGnLuABvU0js7TRlNw4e3ujAPvDrWaZSxEh5mjR4/G6NGjRb+Qt7c3PD09RT+vtmqp4cpHk48e7oAnfozXeyJp2ag+9s4eLEk282vD2iqnmf/37SHKiPu7id2qlW3fxB1LDJwwDND846CaVzFjSBvM2XJS1AgCDxdHTOoTAIWgPmfDuK5++Gx8MNycHc0yv8mUfi2qJS4CwH8Gt8bgwMZo3rC+SXlF+uq4d/ZgZOUVI8j3wcnqlUGt4e7iiA1Hrup8bo/m6ld4744OQtG9MjzRo/qVuUymnl/k7uygt/9ZH2M+xy0a1ccCHcm65vT5Y51wvaAEZ7ILMF4lGdXB3g5Bvm44nVVg0v41BYmmaqanFUyMNo1d4ebsiCMfDrf4SuXaaPpuVSWTyarlwGgaRdimsSt+fK4HcvKLkZZdgPRbd6AQgA83pRpcHx93ucbWBg8XR2ya3g+O9jKM+Xa/cvtneoKpqf1b4uf9F7U+3tTLBcc+GqGWqCuWs6MdiksVGgMawLjuN6nU2BKyXbt2RUlJCYKDg/HJJ5+gXz/p8yVM8Wzf5rhZeE/vnAM9WzRA2mejDBpB0ryh8UPJzMUcCWT6mm0HBTZGj+Ze6NTUA1P6tcDAdo3QspG4fud5Gk4EdjKZQTk7ql4e1ArL9l5Q27Zj1kBsS83EtAGak7xkMhk6+mn7sTHfib15w/rKz8QL/Vri0s0idA/wwtrDGaL35VXfSVQQaYzXw9rh3Q0pAIDFT3U1+/6f6dMce9Kua5wErVcLLyRcuo0nehg2v0OrxvUx1kwTPlly/SUx83p09fdEckauQXNcVH7NdQ0Jn/doMN7fqP/H/OVBrfDXkSsG19MSKkemeLs7qwXShgQj47v6wdvdGc9Wac1WPRdWTebu0MRd7w/9u6ODENbeBxNXHARQMcHdkfRc5eO9WniJHsFSNUfu/2b0x+q4S8puLGtm8WCkSZMm+PHHH9GzZ0+UlJTgp59+wuDBg3Ho0CF076755FhSUoKSkhLl/fz8fEtXUzRHezu8NdKwJCBTh7JaG/VYpPqJ2tHeDn+9+mBSHtW8FXNp4+2KzLxiveXeHRWkDEYqg6hAXzcEikhyM5Qpo8E/ErNOkYmjH40NSJ/uHYB+bRqhqaeLzqveX17ojckrD4ve//AOPtjz1mCNPwIrn++FxEu3ld2dqqoOwZw7PlhHMFnRajLhx3jM1vH9DvZzx59Jhtfd2GMvZmHM9a+EICuvWG8ekS49mnsh6fJtfPRwB0zq0xwBDerBVe6A7/ecx85T2RpnXG7n44a3RwXiy+1pattDWjWEQhDwyuDWmLIqweg6GULs+9stwBNH7wcG8x/vrDGXyViVn3xHezu1ZOLWjV3VgpEhenKdVO16cxCuF5SgVWP1YKStj5vGCzNrZPFgJDAwEIGBD77UoaGhOH/+PBYtWoTffvtN43MiIyPx6aefWrpqVAOkGqn31YQuWLAjTWfyGSDtUMLaM4jRMP8Z3Bo/xJ4HoH1uDUN+CAeZMIOptu5RN2dHZQ6UPs/11f2Z6B7ghbS5o3S2GjzbtznKhYpF4y5cLzLodY1Rde6WSiGtGqJPK/WuOEd7O5MCEaBixueT1/KVk8ANaFtxrH6Y1B2ns/IRrCWIc1a54Lo0fwwKS8rMMjWCpSx7rgd+j7+Mp3sHGBWItPMRP4JI9VQzZ1xHUeee1o1dlUPo6ypJUmp79+6Nc+fOaX08IiICeXl5yltGhvhmaapgSJKYudX0hHCa+Lg746sJXSQdsWQpg+6PrLBUX7+2vb49KgjnP38IG+9P2FWX6ZvR1sHeDlP7t0T7Ju6SBNxrX+qLWWYataaqnpMDerZoUK1ly8nBDp2beRqU5wFUn8TvtWFt4d/AxaDZcA0lZt2hyrlQKrs5vN2c8caIQOVcL4b634x+CA9pLmotpjGdKlbanTagFSb1CcD4rn56A2JbJEnompycjCZNtC+FLJfLIZfX/nUzaqtVKiNdJvb2x3sbK/rxpThpWtvVvyVVDdF83J1x+06p6P2M7dwEbs4O6GiBFWUB3Z8TezuZzhWsDdXIVY4bhSXor6Fbxdapzsxrbm29XXE2p9Cs6+sY4o3h7fDG8HaIuJ9TZA6q6w7pu/xZMrEbEi7dMqlVDqgYKq/vAqfqEP4lz3TDl/c6o77coc50qViC6E99YWGhWqvGxYsXkZycjAYNGiAgIAARERG4evUqfv31VwDAN998g5YtW6Jjx44oLi7GTz/9hN27d+Off/4x339BSq8NayuqL5IeMEeLjphVlH+Y1B0fbErFjCHiks9kMpnGYzypTwDWHEo3OJepJrg5O6CguKza9s0z+mHr8Uw83dtyC4pVamLhEQVVZ939r5HJhJ8/2gl7z+TgyV7++HDzCXNUrZptMwfgbmm56ETv2k7bV3f7rAEoLC6Df4N6oruwxF68ffhwB6zcfxERo9tX2Y/MLPOj1HWi36HExEQMGfJgMpY33ngDADB58mSsXr0amZmZSE9PVz5+7949vPnmm7h69Srq1auHzp07Y+fOnWr7oLrFTuVbLHaaayd7O9wrV5i7SjXmP0Na42RmPsYZMFKjVWNXRGmZtM0Yn40PxuvD25k8Hbg5R4fsenMQnlgaj/Rbd9S2N/V0wTQdaz+Z04whbfBr3CUU3Su3yP6HBHmjd4sG6NTMA9MGtDJqKQIAeKZPAJ6pshq4tiGbxnKwt4ObBeYyGhXsizlbTiK4qWVa6/TTHI2oDou3tKn9W5q0GOG2mQMwevG/ZqyRdREdjAwePFjnFeTq1avV7r/99tt4++23RVeMrJednQz7Zg9BqUIh+gpszbQ+eH1dMuaMM7xPtjZxd3Y0eVVXY2mak0Fq3m7OCGvvg5UHtM+3YGn15Q748okumB51xCL7d7S30zmrpzF2vjEI6beKzNIlVhP8PF1w/JMRZlmU0FbpWnDUFvCTQxYRIGLNG1W9WjTA/nfqZnJk7xYNEHUoXX9BMrtmtXAhQl3aeLuafd0lS9M1s2klS+Wt1YKceTJR7Zugnkyi67uuaeZCesDY9WEM9YiZJtmyOA0foqpL1FubLv6e+OLxToia1kd/Yaox2taWklKrxvXhZG+ndRhzTagNIxJrGltG6jiZTIa4d4fiXpnCoCsXWxT9Ul/cLLyncXEzczJ0WGRd80yfAKw8cNEii72J8VSvAP2FzKxV4/q4cL1IueAbqQtp3RATewdg7WHTWgzN+dMd8/oglJYrzDoRGunHYMQGiB1Lb23cnR2QX1xm9IqrfVuJXzbeWFHT+uD9jamYN177qs5S0xQymfIZauPtipRPRtTqSbAs5f9m9MfFG0Xo6Gfb+QCVHunih6hD6Wih0o07LMgbaw+na1280BDmbEmwt5PB3o6BSE2zvbMD1Tl73hqM01kFCG1dc0GFsUJbN8KetwZLXQ2dVPv1f3mhNzYdvYrXh5s2wVZdG0pqqPpyB4ssmmet+rZqiF1vDlJbRXpYe2/88XKI1eXIkHkxGCGr19BVjn5tatcokrpiULvGJk8URaSq6rTmMpkMvVs20FLaMLaXYVH3MIG1jpFwqRWqIyy5Ci0RkSYMRoiIrED3AE+pq0BkMeymISI1VVeDpdqhjbcbtvy3P7xFzmpcl73/UHss23ceHz3cQeqqkIkYjBARAGDv7MHYd/YGnuzZTOqqkBZMhlU3bWArvDigJWTsn7Z6DEbqGPb3k7GaN6yP5xpadq4VInOri4FIMy/jZrC2ZgxGiIiIaoHfp/ZBbFoOJoe2kLoqNY7BCBERUS3Qv20j9G/bSOpqSIKjaYiIiEhSDEbqmPZN3KSuAhERkSjspqkjtr42AKnX8jDcyldXJSIi28NgpI7o4OeODlyMi4iIrBC7aYiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSVrFqryAIAID8/HyJa0JERESGqvzdrvwd18YqgpGCggIAgL+/v8Q1ISIiIrEKCgrg4eGh9XGZoC9cqQUUCgWuXbsGNzc3yGQys+03Pz8f/v7+yMjIgLu7u9n2S+bF42QdeJysB4+VdagLx0kQBBQUFMDPzw92dtozQ6yiZcTOzg7NmjWz2P7d3d2t9kDbEh4n68DjZD14rKyDtR8nXS0ilZjASkRERJJiMEJERESSsulgRC6X4+OPP4ZcLpe6KqQDj5N14HGyHjxW1sGWjpNVJLASERFR3WXTLSNEREQkPQYjREREJCkGI0RERCQpBiNEREQkKZsORr7//nu0aNECzs7O6NOnDw4fPix1leqEyMhI9OrVC25ubvD29sb48eORlpamVqa4uBjTp09Hw4YN4erqiscffxzZ2dlqZdLT0zFmzBjUq1cP3t7emD17NsrKytTKxMbGonv37pDL5WjTpg1Wr15drT48zoaZP38+ZDIZZs2apdzG41R7XL16Fc8++ywaNmwIFxcXdOrUCYmJicrHBUHARx99hCZNmsDFxQVhYWE4e/as2j5u3bqFSZMmwd3dHZ6enpg6dSoKCwvVyhw/fhwDBgyAs7Mz/P398eWXX1ary/r16xEUFARnZ2d06tQJW7dutcw/bWXKy8vx4YcfomXLlnBxcUHr1q0xd+5ctXVZeJy0EGxUdHS04OTkJKxcuVI4ceKEMG3aNMHT01PIzs6WumpWb+TIkcKqVauE1NRUITk5WXjooYeEgIAAobCwUFnmlVdeEfz9/YVdu3YJiYmJQt++fYXQ0FDl42VlZUJwcLAQFhYmHD16VNi6davQqFEjISIiQlnmwoULQr169YQ33nhDOHnypPDdd98J9vb2wvbt25VleJwNc/jwYaFFixZC586dhZkzZyq38zjVDrdu3RKaN28uPP/888KhQ4eECxcuCDt27BDOnTunLDN//nzBw8ND2LRpk3Ds2DHhkUceEVq2bCncvXtXWWbUqFFCly5dhIMHDwr//vuv0KZNG2HixInKx/Py8gQfHx9h0qRJQmpqqrB27VrBxcVFWLZsmbLMgQMHBHt7e+HLL78UTp48KXzwwQeCo6OjkJKSUjNvRi02b948oWHDhsKWLVuEixcvCuvXrxdcXV2FxYsXK8vwOGlms8FI7969henTpyvvl5eXC35+fkJkZKSEtaqbcnJyBADC3r17BUEQhNzcXMHR0VFYv369ssypU6cEAEJ8fLwgCIKwdetWwc7OTsjKylKWWbp0qeDu7i6UlJQIgiAIb7/9ttCxY0e113rqqaeEkSNHKu/zOOtXUFAgtG3bVoiJiREGDRqkDEZ4nGqPd955R+jfv7/WxxUKheDr6yssWLBAuS03N1eQy+XC2rVrBUEQhJMnTwoAhISEBGWZbdu2CTKZTLh69aogCILwww8/CF5eXspjV/nagYGByvtPPvmkMGbMGLXX79Onj/Dyyy+b9k/WAWPGjBFeeOEFtW2PPfaYMGnSJEEQeJx0sclumnv37iEpKQlhYWHKbXZ2dggLC0N8fLyENaub8vLyAAANGjQAACQlJaG0tFTt/Q8KCkJAQIDy/Y+Pj0enTp3g4+OjLDNy5Ejk5+fjxIkTyjKq+6gsU7kPHmfDTJ8+HWPGjKn2XvI41R7/+9//0LNnT0yYMAHe3t7o1q0bVqxYoXz84sWLyMrKUnsPPTw80KdPH7Vj5enpiZ49eyrLhIWFwc7ODocOHVKWGThwIJycnJRlRo4cibS0NNy+fVtZRtfxtGWhoaHYtWsXzpw5AwA4duwY9u/fj9GjRwPgcdLFKhbKM7cbN26gvLxc7QQKAD4+Pjh9+rREtaqbFAoFZs2ahX79+iE4OBgAkJWVBScnJ3h6eqqV9fHxQVZWlrKMpuNT+ZiuMvn5+bh79y5u377N46xHdHQ0jhw5goSEhGqP8TjVHhcuXMDSpUvxxhtv4L333kNCQgJee+01ODk5YfLkycr3WtN7qHocvL291R53cHBAgwYN1Mq0bNmy2j4qH/Py8tJ6PCv3Ycveffdd5OfnIygoCPb29igvL8e8efMwadIkAOBx0sEmgxGqOdOnT0dqair2798vdVWoioyMDMycORMxMTFwdnaWujqkg0KhQM+ePfH5558DALp164bU1FT8+OOPmDx5ssS1o0p//PEH1qxZg6ioKHTs2BHJycmYNWsW/Pz8eJz0sMlumkaNGsHe3r7aqIDs7Gz4+vpKVKu6Z8aMGdiyZQv27NmDZs2aKbf7+vri3r17yM3NVSuv+v77+vpqPD6Vj+kq4+7uDhcXFx5nPZKSkpCTk4Pu3bvDwcEBDg4O2Lt3L7799ls4ODjAx8eHx6mWaNKkCTp06KC2rX379khPTwfw4L3W9R76+voiJydH7fGysjLcunXLLMeTxwqYPXs23n33XTz99NPo1KkTnnvuObz++uuIjIwEwOOki00GI05OTujRowd27dql3KZQKLBr1y6EhIRIWLO6QRAEzJgxAxs3bsTu3burNSf26NEDjo6Oau9/Wloa0tPTle9/SEgIUlJS1L6UMTExcHd3V56UQ0JC1PZRWaZyHzzOug0bNgwpKSlITk5W3nr27IlJkyYp/+Zxqh369etXbXj8mTNn0Lx5cwBAy5Yt4evrq/Ye5ufn49ChQ2rHKjc3F0lJScoyu3fvhkKhQJ8+fZRl9u3bh9LSUmWZmJgYBAYGwsvLS1lG1/G0ZXfu3IGdnfrPqr29PRQKBQAeJ52kzqCVSnR0tCCXy4XVq1cLJ0+eFF566SXB09NTbVQAGefVV18VPDw8hNjYWCEzM1N5u3PnjrLMK6+8IgQEBAi7d+8WEhMThZCQECEkJET5eOWQ0REjRgjJycnC9u3bhcaNG2scMjp79mzh1KlTwvfff69xyCiPs+FUR9MIAo9TbXH48GHBwcFBmDdvnnD27FlhzZo1Qr169YTff/9dWWb+/PmCp6ensHnzZuH48ePCuHHjNA4Z7datm3Do0CFh//79Qtu2bdWGjObm5go+Pj7Cc889J6SmpgrR0dFCvXr1qg0ZdXBwEL766ivh1KlTwscff1yrh4zWpMmTJwtNmzZVDu3dsGGD0KhRI+Htt99WluFx0sxmgxFBEITvvvtOCAgIEJycnITevXsLBw8elLpKdQIAjbdVq1Ypy9y9e1f4z3/+I3h5eQn16tUTHn30USEzM1NtP5cuXRJGjx4tuLi4CI0aNRLefPNNobS0VK3Mnj17hK5duwpOTk5Cq1at1F6jEo+z4aoGIzxOtcf//d//CcHBwYJcLheCgoKE5cuXqz2uUCiEDz/8UPDx8RHkcrkwbNgwIS0tTa3MzZs3hYkTJwqurq6Cu7u7MGXKFKGgoECtzLFjx4T+/fsLcrlcaNq0qTB//vxqdfnjjz+Edu3aCU5OTkLHjh2Fv//+2/z/sBXKz88XZs6cKQQEBAjOzs5Cq1athPfff19tCC6Pk2YyQVCZGo6IiIiohtlkzggRERHVHgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhSDEaIiIhIUgxGiIiISFIMRoiIiEhS/w8G3Afn0f+rEgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "001cef78-9796-48c4-b196-b422f0a2040c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([161132, 137763, 156815, 221449, 101514, 224675,  58657, 226139,  13088,\n",
       "        225364, 154831,  31222, 174983, 134401, 131670,  41684,  64865, 116922,\n",
       "         90102, 187389,  61106,  67555,  61360, 180903, 161685, 176190, 128910,\n",
       "         72755,  47026, 133884, 136604, 140088])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This seems to be taking too long, we can instead move onto the usage within miniBatches (random)\n",
    "torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "# with this I will add a STEP2 to the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "fb9be348-98a8-49c2-bbed-99085a96ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gurinah.\n",
      "zubilia.\n",
      "syze.\n",
      "nelie.\n",
      "taia.\n",
      "mayam.\n",
      "kamots.\n",
      "keemon.\n",
      "dalxa.\n",
      "mitel.\n",
      "zurav.\n",
      "hodee.\n",
      "browiy.\n",
      "deace.\n",
      "madie.\n",
      "caerleiga.\n",
      "dayey.\n",
      "eilva.\n",
      "saysiel.\n",
      "sanarrus.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * blockSize # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      prob = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(prob, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "62b4dacb-7587-4302-884d-47245ec75623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i didn't get into building train set and the loss diagrams, that is that is mising from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f853ab-1880-4d75-915c-efbc689f62f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
